{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71efa416",
   "metadata": {},
   "source": [
    "## 1. Извлечение ключевых фраз из текстов\n",
    "- **a. Классический метод**  \n",
    "    Использован алгоритм:  \n",
    "    - YAKE  \n",
    "- **b. Метод с использованием языковой модели (LLM)**  \n",
    "    Была использована модель:\n",
    "    - Google Gemini 2.5 Flash Lite по API.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Построение системы поиска\n",
    "\n",
    "- **Вариант 1:** Поиск на основе TF-IDF\n",
    "- **Вариант 2:** Поиск на основе эмбеддингов\n",
    "\n",
    "### Функция поиска\n",
    "- **Вход:** текстовый запрос  \n",
    "- **Выход:** топ-5 релевантных аннотаций\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Сравнение подходов\n",
    "\n",
    "- Примеры извлечённых ключевых фраз для каждого метода\n",
    "- Сравнение результатов поиска по двум вариантам\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2da0c",
   "metadata": {},
   "source": [
    "> Данная книга является только предоставлением результатов (итоговым отчетом), основные этапы и полный код можно найти в последующих ноутбуках по нумерации"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf16fd4c",
   "metadata": {},
   "source": [
    "# Часть 1: Извлечение ключевых фраз (Задача 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8897ab",
   "metadata": {},
   "source": [
    "**Классический метод (YAKE)**\n",
    "YAKE анализирует текст, выявляя слова и фразы, которые статистически выделяются на фоне остального текста. Находит наиболее значимые фразы в тексте, которые отражают его основное содержание."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b994adc4",
   "metadata": {},
   "source": [
    "* **Метод с использованием LLM.**\n",
    "Метод основанный на обращении к большой языковой модели подразумевает более качественный вывод ключевых слов, благодаря способности модели обобщать и улавливать семантический смысл текстов.\n",
    "* **Методы реализации** \n",
    "Реализация этого метода была осуществлена с помощью обращения к Google Gemini Flash 2.5 Lite по API. Она является быстрой, дешевой и соотвествует обработки больших корпусов текста, что прямо упомянуто в документации к модели. Дополнительное преимущество: Structured Output позволяют сразу распарсить ответ модели и не тратить лишние токены на просьбы вывести в json структуре с примерами.\n",
    "Ввиду ограничений в количестве запросов во free tier плане гугл и размере данных было приянто решение сократить выборку до 200 случайных ед аннотаций, однако реализованные методы являются масштабируемыми для любого количества аннотаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb4097af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc2a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные успешно загружены из ../data/df_with_llmkeyphrases.parquet. Количество записей: 200\n"
     ]
    }
   ],
   "source": [
    "from src.utils import load_dataset\n",
    "df = load_dataset(\"df_with_llmkeyphrases\")\n",
    "df_keys = df[[\"abstract\", \"yake_keywords\", \"keyphrases\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840409e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "abstract",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "yake_keywords",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "keyphrases",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a35787ce-98dc-4496-8125-deaf3b59a42c",
       "rows": [
        [
         "0",
         "Представлен усовершенствованный метод парных сравнений, в котором посредством табличных форм систематизированы правила логических выводов при сравнении технических систем и формулы проверочных значений. Для этого сформулированы рациональные правила логических выводов при парном сравнении систем. С целью проверки результатов оценки на непротиворечивость введены понятия количества баллов, набранных одной системой, и коэффициента качества систем, а также разработаны формулы расчетов. Для целей практического использования данного метода при разработке программ для ЭВМ предлагаются формализованные варианты взаимосвязанных таблиц: таблица обработки и систематизации экспертной информации, таблица возможных логических выводов по результатам сравнения заданного количества технических систем и таблица проверочных значений при использовании метода парных сравнений при оценке качества определенного количества технических систем. Таблицы позволяют более рационально организовать процедуры обработки информации и в значительной степени исключить влияние ошибок при вводе данных на результаты оценки качества технических систем. Основной положительный эффект от внедрения усовершенствованного метода парных сравнений состоит в существенном сокращении времени и ресурсов на организацию работы с экспертами, обработку экспертной информации, а также на подготовку и проведение дистанционного опроса экспертов по сети Интернет или локальной вычислительной сети предприятия (организации) за счет рационального использования исходных данных о качестве оцениваемых систем. Предлагаемый усовершенствованный метод реализован в программах для ЭВМ, предназначенных для оценки эффективности и устойчивости больших технических систем.",
         "['котором посредством табличных' 'посредством табличных форм'\n 'табличных форм систематизированы' 'правила логических выводов'\n 'форм систематизированы правила' 'систематизированы правила логических'\n 'логических выводов']",
         "['усовершенствованный метод парных сравнений' 'логические выводы'\n 'коэффициент качества систем' 'обработка экспертной информации'\n 'программы для ЭВМ']"
        ],
        [
         "1",
         "Статья посвящена разработке комплексной модели диктора в задаче текстонезависимой идентификации по голосу. Комплексная модель базируется на методе гауссовых смесей. Ее формируют по речевому сигналу, который предварительно сегментируется на фрагменты, соответствующие различным фонетическим классам звуков. Предложен способ структурирования моделей дикторов. Модели дикторов структурированы в виде дерева, что позволило проводить идентификацию диктора без выполнения полного перебора всего множества моделей. Проведенные исследования показали, что деление акустического пространства голоса диктора на множество классов, представляющих некоторые фонетические события, приводит к увеличению эффективности идентификации по голосу, а предложенное структурирование множества моделей дикторов ускоряет операцию поиска.",
         "['Статья посвящена разработке' 'посвящена разработке комплексной'\n 'Статья посвящена' 'задаче текстонезависимой идентификации'\n 'разработке комплексной модели' 'посвящена разработке'\n 'разработке комплексной']",
         "['комплексная модель диктора' 'текстонезависимая идентификация по голосу'\n 'метод гауссовых смесей' 'структурирование моделей дикторов'\n 'эффективность идентификации']"
        ],
        [
         "2",
         "В работе рассмотрена обобщенная модель образования новой фазы, объединяющая три основные стадии процесса роста при фазовом переходе первого рода. Получено численное решение кинетического уравнения Фоккера–Планка. Исследована зависимость решения от параметров системы, выявлены области применимости допущений, сделанных Зельдовичем, Лифшицем и Слезовым, и показано, что в зависимости от параметров системы можно получить как равновесное распределение, так и автомодельное распределение \r\nЛифшица–Слезова. При некоторых значениях параметров уравнение имеет осциллирующее решение.",
         "['образования новой фазы' 'переходе первого рода'\n 'работе рассмотрена обобщенная' 'рассмотрена обобщенная модель'\n 'обобщенная модель образования' 'модель образования новой'\n 'основные стадии процесса']",
         "['образование новой фазы' 'фазовый переход первого рода'\n 'кинетическое уравнение Фоккера–Планка' 'равновесное распределение'\n 'автомодельное распределение']"
        ],
        [
         "3",
         "Предложено обобщение класса экспоненциально-степенных распределений (обобщенных распределений Лапласа) на несимметричный случай. Класс скошенных экспоненциально-степенных распределений (скошенных обобщенных распределений Лапласа) вводится как семейство специальных дисперсионно-сдвиговых смесей нормальных законов. Найдены выражения для моментов скошенных экспоненциально-степенных распределений. Показано, что скошенные экспоненциально-степенные распределения могут использоваться в качестве асимптотических аппроксимаций. С этой целью доказывается теорема о необходимых и достаточных условиях сходимости распределений сумм случайного числа независимых одинаково распределенных случайных величин к скошенным экспоненциально-степенным распределениям. Для частного случая — специальных случайных блужданий с непрерывным временем, порожденных обобщенными дважды стохастическими пуассоновскими процессами, — приводятся оценки скорости этой сходимости.",
         "['обобщенных распределений Лапласа' 'Предложено обобщение класса'\n 'обобщение класса экспоненциально-степенных' 'распределений Лапласа'\n 'класса экспоненциально-степенных распределений'\n 'скошенных экспоненциально-степенных распределений'\n 'Предложено обобщение']",
         "['экспоненциально-степенные распределения'\n 'обобщенные распределения Лапласа' 'несимметричный случай'\n 'асимптотические аппроксимации' 'сходимость распределений сумм']"
        ],
        [
         "4",
         "Рассматривается задача объединения графов с общей частью, которые были получены в результате серии моделирований сети Петри с использованием программного пакета Colored Petri Nets Tools, в котором адресное пространство процесса ограничено 2<nobr>$^{32}$</nobr> байтами, начиная с различных вершин и при различных начальных условиях. Для ее решения необходимо определить общую часть графов, выполнить разрез таким образом, чтобы их общая часть осталась только в одном из начальных графов, и составить таблицу соответствия (переходов) между вершинами графов для возможности осуществления переходов между ними. Изначально предполагается, что графы представлены в виде списков смежности, но в процессе работы алгоритма они преобразовываются в хеш-таблицы для быстрого определения общей части графов, которое реализуется при помощи обхода одного из графов и проверки наличия вершин во втором. Составление таблицы переходов между графами осуществляется при помощи обхода графа по парам «родительская-дочерняя» вершины, в ходе которого проверяются условия добавления узлов в таблицу переходов. Предлагается алгоритм решения задачи объединения частей ориентированного графа и приведен пример его использования.",
         "['Petri Nets Tools' 'Colored Petri Nets' 'пакета Colored Petri'\n 'моделирований сети Петри' 'программного пакета Colored' 'Nets Tools'\n 'пространство процесса ограничено']",
         "['Объединение графов' 'Сети Петри' 'Общая часть графов'\n 'Алгоритм объединения' 'Хеш-таблицы']"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>yake_keywords</th>\n",
       "      <th>keyphrases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Представлен усовершенствованный метод парных с...</td>\n",
       "      <td>['котором посредством табличных' 'посредством ...</td>\n",
       "      <td>['усовершенствованный метод парных сравнений' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Статья посвящена разработке комплексной модели...</td>\n",
       "      <td>['Статья посвящена разработке' 'посвящена разр...</td>\n",
       "      <td>['комплексная модель диктора' 'текстонезависим...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>В работе рассмотрена обобщенная модель образов...</td>\n",
       "      <td>['образования новой фазы' 'переходе первого ро...</td>\n",
       "      <td>['образование новой фазы' 'фазовый переход пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Предложено обобщение класса экспоненциально-ст...</td>\n",
       "      <td>['обобщенных распределений Лапласа' 'Предложен...</td>\n",
       "      <td>['экспоненциально-степенные распределения'\\n '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Рассматривается задача объединения графов с об...</td>\n",
       "      <td>['Petri Nets Tools' 'Colored Petri Nets' 'паке...</td>\n",
       "      <td>['Объединение графов' 'Сети Петри' 'Общая част...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            abstract  \\\n",
       "0  Представлен усовершенствованный метод парных с...   \n",
       "1  Статья посвящена разработке комплексной модели...   \n",
       "2  В работе рассмотрена обобщенная модель образов...   \n",
       "3  Предложено обобщение класса экспоненциально-ст...   \n",
       "4  Рассматривается задача объединения графов с об...   \n",
       "\n",
       "                                       yake_keywords  \\\n",
       "0  ['котором посредством табличных' 'посредством ...   \n",
       "1  ['Статья посвящена разработке' 'посвящена разр...   \n",
       "2  ['образования новой фазы' 'переходе первого ро...   \n",
       "3  ['обобщенных распределений Лапласа' 'Предложен...   \n",
       "4  ['Petri Nets Tools' 'Colored Petri Nets' 'паке...   \n",
       "\n",
       "                                          keyphrases  \n",
       "0  ['усовершенствованный метод парных сравнений' ...  \n",
       "1  ['комплексная модель диктора' 'текстонезависим...  \n",
       "2  ['образование новой фазы' 'фазовый переход пер...  \n",
       "3  ['экспоненциально-степенные распределения'\\n '...  \n",
       "4  ['Объединение графов' 'Сети Петри' 'Общая част...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_keys = pd.read_csv(\"../data/selected_examples.csv\")\n",
    "df_keys.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7398b",
   "metadata": {},
   "source": [
    "keyphrases - словосочетания извлеченные LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43be505",
   "metadata": {},
   "source": [
    "Я отобрал примеры, я вывел их в отдельный selected_samples файл, так как выборка из 200 абстрактов выполняется случайно, я не стал ставить конкретные сиды на генерацию и на отбор. Тем не менее выбранные показатели довольно легко находятся при беглом просмотре с любой конфигурацией, ведь паттерн одинаков, как и соотвествующие выовды:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c18b81d",
   "metadata": {},
   "source": [
    "1) +LLM Семантика. LLM выдает чистые и осмысленные концепции, в то время как YAKE давал много артефактов, включая академический шум в виде \"в статье рассматрвиается\"\n",
    "2) +LLM LLM хорошо обобщает и находит релевантные ключевые фразы. Статистический метод на это не способен\n",
    "3) +Yake YAKE! все таки смог найти некоторые четкие уникальные термины, это видно в последнем пятом примере (Petri Nets Tools), однако эти уникальные термины могут быть лишь просто уникальным упоминанием, а не действительно ключевым словом.\n",
    "4) +YAKE Статистический метод справляется с 100 и более аннотациями в секунду и стоит практически ничего в денежном эквиваленте, в то время как использование LLM при моих параметрах батчинга 20 и 10 одновременных запросах способен только на примерно 10 аннотаций в секунду\n",
    "5) +YAKE реализация этого метода довольно проста, в то время как для LLM нужно построить небольшой пайплайн. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eba407",
   "metadata": {},
   "source": [
    "| Критерий | YAKE (Классический подход) | LLM (Gemini 2.5 Flash Lite) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Качество фраз** | Низкое-Среднее (извлекает n-граммы, шум, \"рваные\" фразы) | Высокое (извлекает семантические концепции, обобщает) |\n",
    "| **Скорость** | Очень высокая (<1с на ~200 док.) | Низкая (20с на 200 док.) |\n",
    "| **Стоимость** | 0 (бесплатно) | Низкая, но не нулевая (~$0.01 за 200 док.) |\n",
    "| **Сложность реализации** | Низкая (импорт библиотеки) | Средняя (требует инженерии API, обработки ошибок) |\n",
    "| **Сценарий использования**| Быстрый baseline, поиск по точным терминам, прототипирование. | Production-системы, семантический анализ, задачи, где качество важнее скорости. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77909e4",
   "metadata": {},
   "source": [
    "Выбор метода зависит от задачи: для быстрого прототипа и простого извлечения терминов YAKE является адекватным baseline, но для построения качественной системы, понимающей смысл текста, LLM-подход является одним из лучших"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7be8443",
   "metadata": {},
   "source": [
    "# Часть 2: Семантический поиск"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29ff04",
   "metadata": {},
   "source": [
    "* Оба метода предстовляют собой векторный поиск, разниц лишь в том как векторы создаются.\n",
    "### **TF-IDF**\n",
    "Насколько важным является каждое конкретное слово из предопределенного словаря для данного текста. Это **явное (explicit)** представление, где каждая размерность вектора соответствует одному слову. СОздается путем подсчета статистики слов по всему корпусу, вектор зависит от остальных документов в коллекции.\n",
    "### **Эмбиддинги**\n",
    "Векторы текстов создаются трансформером предобученной на больших объемах текста чтобы понимать контекст и семантические связи между словами. Вектор для одного документа **не зависит от других документов в твоей коллекции** - он вычисляется только на основе самого текста и знаний модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09160b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.search_tf_idf.search import TfidfSearch\n",
    "from src.search_embeddings.engine import EmbeddingSearchEngine\n",
    "from src.classic_keywords.preprocessing import preprocess_text, lemmatize_word\n",
    "from IPython.display import display, Markdown\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61df3636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Инициализация TF-IDF...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lemmatized_abstract'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/tech-task-dfs/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmatized_abstract'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mИнициализация TF-IDF...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tfidf_search_engine \u001b[38;5;241m=\u001b[39m TfidfSearch()\n\u001b[1;32m      3\u001b[0m tfidf_search_engine\u001b[38;5;241m.\u001b[39mbuild_index(\n\u001b[0;32m----> 4\u001b[0m     lemmatized_texts\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlemmatized_abstract\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m,\n\u001b[1;32m      5\u001b[0m     original_texts\u001b[38;5;241m=\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabstract\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tech-task-dfs/lib/python3.11/site-packages/pandas/core/frame.py:4107\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4107\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4109\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tech-task-dfs/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'lemmatized_abstract'"
     ]
    }
   ],
   "source": [
    "print(\"Инициализация TF-IDF...\")\n",
    "tfidf_search_engine = TfidfSearch()\n",
    "tfidf_search_engine.build_index(\n",
    "    lemmatized_texts=df['lemmatized_abstract'],\n",
    "    original_texts=df['abstract']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89abbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nИнициализация Embedding Search...\")\n",
    "embedding_search_engine = EmbeddingSearchEngine()\n",
    "embedding_search_engine.build_index(df['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474cb42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_keywords(text: str, query: str, preprocessor_func) -> str:\n",
    "    query_lemmas = set(preprocessor_func(query).split())\n",
    "    tokens = re.split(r'(\\s+)', text) \n",
    "    highlighted_tokens = []\n",
    "    for token in tokens:\n",
    "        if not token.strip():\n",
    "            highlighted_tokens.append(token)\n",
    "            continue\n",
    "        cleaned_token = token.strip('.,!?:;()[]{}')\n",
    "        if lemmatize_word(cleaned_token.lower()) in query_lemmas:\n",
    "            highlighted_tokens.append(f\"**{token}**\")\n",
    "        else:\n",
    "            highlighted_tokens.append(token)\n",
    "    return \"\".join(highlighted_tokens)\n",
    "\n",
    "print(\"\\nПодготовка завершена. Все поисковые системы готовы к работе.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3a2905",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"высокотехнологичная медицинская помощь сердечно-сосудистая хирургия ДВФО\"\n",
    "\n",
    "tfidf_results1 = tfidf_search_engine.search(query1, preprocessor_func=preprocess_text)\n",
    "embedding_results1 = embedding_search_engine.search(query1, top_n=5)\n",
    "\n",
    "tfidf_md = \"\"\n",
    "for i, (doc_index, doc_text) in enumerate(tfidf_results1, 1):\n",
    "    highlighted = highlight_keywords(doc_text, query1, preprocess_text)\n",
    "    tfidf_md += f\"**{i}. Индекс {doc_index}** <br> `{highlighted[:250].replace('*', '')}...` <br><br>\"\n",
    "\n",
    "embed_md = \"\"\n",
    "for i, (doc_index, doc_text, score) in enumerate(embedding_results1, 1):\n",
    "    embed_md += f\"**{i}. Индекс {doc_index} (Score: {score:.3f})** <br> `{doc_text[:250]}...` <br><br>\"\n",
    "\n",
    "comparison_table = f\"\"\"\n",
    "| TF-IDF (Лексический поиск) | Семантический поиск (FRIDA) |\n",
    "| :--- | :--- |\n",
    "| {tfidf_md} | {embed_md} |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93644491",
   "metadata": {},
   "source": [
    "Сравним результаты: запрос был выбран специально с включением ключевых слов и легко можно заметить оба поисковых метода смогли верно определить таргет аннотацию. В целом такой результат и ожидался.\n",
    "> Ключевой момент: поиск TF занял **0.033547 секунд**, в то время как эмбиддинг справился за **1.29 секунд**. \n",
    "Это показывает разницу: статистический метод молниеносен, он не нагружает систему и не требует больших вычислений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc8bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"кардиохирургия ВМП Дальний Восток\"\n",
    "\n",
    "tfidf_results1 = tfidf_search_engine.search(query1, preprocessor_func=preprocess_text)\n",
    "embedding_results1 = embedding_search_engine.search(query1, top_n=5)\n",
    "\n",
    "tfidf_md = \"\"\n",
    "for i, (doc_index, doc_text) in enumerate(tfidf_results1, 1):\n",
    "    highlighted = highlight_keywords(doc_text, query1, preprocess_text)\n",
    "    tfidf_md += f\"**{i}. Индекс {doc_index}** <br> `{highlighted[:250].replace('*', '')}...` <br><br>\"\n",
    "\n",
    "embed_md = \"\"\n",
    "for i, (doc_index, doc_text, score) in enumerate(embedding_results1, 1):\n",
    "    embed_md += f\"**{i}. Индекс {doc_index} (Score: {score:.3f})** <br> `{doc_text[:250]}...` <br><br>\"\n",
    "\n",
    "comparison_table = f\"\"\"\n",
    "| TF-IDF (Лексический поиск) | Семантический поиск (FRIDA) |\n",
    "| :--- | :--- |\n",
    "| {tfidf_md} | {embed_md} |\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(comparison_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a68f0d",
   "metadata": {},
   "source": [
    "Это пример нового поискового запроса с целью на ту же самую аннотацию, однако теперь запрос был переформулирован синонимично, с сохранением смысла.\n",
    "> Легко заметить что эмбиддинг справился, в то время как лексический поиск полностью провалился не предоставив аннотацию даже в пятерке. \n",
    "\n",
    "Здесь и предоставляется снова нюанс: предобученные модели качественно справляются со своей задачей часто гораздо лучше чем любой другой статистический метод основанный на грубых вычислениях, однако они входят в компромис с вычислениями, история похожая на сравнения извлечения ключевых слов\n",
    "Кроме того, вычисление эмбиддингов тоже требует вычислений, для хорошего результата с использованием лучших моделей время сильно зависит от мощностей."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-task-dfs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
