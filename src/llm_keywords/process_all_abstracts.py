import os
import time
import asyncio
import pandas as pd
import pandas as pd
from google import genai
from .extracrt_keyphrases import RateLimiter
from .extracrt_keyphrases import extract_keyphrases_batch_async

async def process_all_abstracts_async(df: pd.DataFrame, batch_size: int = 20, 
                                    max_concurrent: int = 5, 
                                    max_retries: int = 3) -> pd.DataFrame:
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏ –±–∞—Ç—á–∞–º–∏ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏"""
    
    client = genai.Client(api_key=os.environ.get("GEMINI_API_KEY"))
    rate_limiter = RateLimiter(max_requests_per_minute=14)
    
    df = df.copy()
    df['keyphrases'] = None
    df['keyphrases'] = df['keyphrases'].astype('object')
    
    abstracts = df['abstract'].tolist()
    total_batches = (len(abstracts) + batch_size - 1) // batch_size
    
    print(f"–ë—É–¥–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_batches} –±–∞—Ç—á–µ–π –ø–æ {batch_size} –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π")
    print(f"–ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {max_concurrent}")
    print(f"–ú–∞–∫—Å–∏–º—É–º –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫: {max_retries}")
    
    tasks = []
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(abstracts))
        current_batch = abstracts[start_idx:end_idx]
        
        task = extract_keyphrases_batch_async(
            client, current_batch, start_idx, batch_idx, rate_limiter, 
            max_retries=max_retries
        )
        tasks.append(task)
    
    semaphore = asyncio.Semaphore(max_concurrent)
    
    async def bounded_task(task):
        async with semaphore:
            return await task
    
    bounded_tasks = [bounded_task(task) for task in tasks]
    
    print("\nüöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
    start_time = time.time()
    
    results = await asyncio.gather(*bounded_tasks, return_exceptions=True)
    
    end_time = time.time()
    
    successful_batches = 0
    failed_batches = 0
    retry_stats = {}
    
    for result in results:
        if isinstance(result, Exception):
            print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –≤ –æ–¥–Ω–æ–º –∏–∑ –±–∞—Ç—á–µ–π: {result}")
            failed_batches += 1
            continue
            
        batch_idx, parsed_result = result
        if parsed_result:
            successful_batches += 1
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(abstracts))
            
            for i, idx in enumerate(range(start_idx, end_idx)):
                key = f"annotation_{idx + 1}"
                if key in parsed_result:
                    keyphrases_list = parsed_result[key]
                    df.at[idx, 'keyphrases'] = keyphrases_list
        else:
            failed_batches += 1
    
    print(f"\n=== üìä –î–ï–¢–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful_batches}/{total_batches} –±–∞—Ç—á–µ–π")
    print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {failed_batches}/{total_batches} –±–∞—Ç—á–µ–π")
    print(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {successful_batches/total_batches*100:.1f}%")
    print(f"\n‚è±Ô∏è  –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {end_time - start_time:.1f} —Å–µ–∫—É–Ω–¥")
    
    return df